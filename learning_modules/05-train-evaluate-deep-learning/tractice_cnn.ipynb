{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here are the key topic sentences extracted from the Microsoft Learn page on Convolutional Neural Networks (CNNs):\n",
    "\n",
    "* Deep learning models are particularly useful for data consisting of large arrays of numeric values, such as images.\n",
    "* At the heart of deep learning‚Äôs success in computer vision is the convolutional neural network (CNN).\n",
    "* CNNs consist of multiple layers, each performing a specific task in extracting features or predicting labels.\n",
    "* The break down layer types (convolution, pooling, dropping, flattening, fully connected).\n",
    "    * A convolutional layer extracts important features in images by applying a filter defined by a kernel.\n",
    "    * Pooling layers reduce the number of feature values while retaining key differentiating features.\n",
    "    * Dropping layers help mitigate overfitting by randomly eliminating feature maps during training.\n",
    "    * Flattening layers convert multidimensional feature maps into a vector for input to a fully connected layer.\n",
    "    * Fully connected layers generate predictions by passing feature values through hidden layers to an output layer.\n",
    "* A CNN is trained by passing batches of data through multiple epochs, adjusting weights via backpropagation."
   ],
   "id": "443cbdd023eda9b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "torch_data_dir = '../../generated/data/torch'\n",
    "os.makedirs(torch_data_dir, exist_ok=True)"
   ],
   "id": "253bef6ad1cb6b6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Example: CNN Training with Weight & Bias Updates",
   "id": "c43591fab3a77238"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 1. Define a simple CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layer: input channels=1 (grayscale), output channels=10, kernel=3x3\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(10*26*26, 10)  # 28x28 image -> 26x26 after 3x3 conv\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))   # Apply convolution + ReLU\n",
    "        x = x.view(-1, 10*26*26)    # Flatten\n",
    "        x = self.fc1(x)             # Fully connected\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 2. Load dataset (MNIST digits)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(torch_data_dir, train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "\n",
    "# 3. Initialize model, optimizer, and loss function\n",
    "model = SimpleCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. Training loop\n",
    "for epoch in range(3):  # run for 3 epochs\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()              # Reset gradients\n",
    "        output = model(data)               # Forward pass\n",
    "        loss = loss_fn(output, target)     # Compute loss\n",
    "        loss.backward()                    # Backpropagation (compute gradients)\n",
    "        optimizer.step()                   # Update weights & biases\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader)}\")\n",
    "\n",
    "    # Inspect weight & bias updates (example: first conv layer)\n",
    "    print(\"Conv1 Weights (first filter):\", model.conv1.weight[0][0][:5,:5])\n",
    "    print(\"Conv1 Bias:\", model.conv1.bias.data)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Explanation of Weight & Bias Updates\n",
    "* Forward pass: Input images go through convolution ‚Üí ReLU ‚Üí flatten ‚Üí fully connected ‚Üí softmax.\n",
    "* Loss calculation: CrossEntropy compares predictions vs. true labels.\n",
    "* Backward pass: loss.backward() computes gradients for each weight and bias.\n",
    "* Update step: optimizer.step() adjusts weights and biases using gradient descent.\n",
    "* Inspection: After each epoch, you can print out the weights and biases to see how they change."
   ],
   "id": "3ad4ffdc6d577639"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìà Expected Outcomes of Increasing Epochs\n",
    "* Training Loss Decreases (at first):\n",
    "    * With more epochs, the model sees the data multiple times.\n",
    "    * Weights and biases are adjusted repeatedly, so the loss usually drops further compared to fewer epochs.\n",
    "* Accuracy Improves (up to a point):\n",
    "    * The model learns more features and patterns, so accuracy on the training set and often the validation set increases.\n",
    "    * Example: Going from 3 epochs to 10 epochs might raise accuracy from ~85% to ~95%.\n",
    "* Risk of Overfitting:\n",
    "    * After a certain number of epochs, the model may start memorizing training data instead of generalizing.\n",
    "    * Training accuracy keeps climbing, but validation/test accuracy may plateau or even decline.\n",
    "* Weight & Bias Adjustments:\n",
    "    * Early epochs ‚Üí large changes in weights (big gradient steps).\n",
    "    * Later epochs ‚Üí smaller refinements as the optimizer converges.\n",
    "    * Eventually, updates become tiny because the model is close to a minimum in the loss function."
   ],
   "id": "bb60e6ae4e52d18a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üîÆ Assumed Example Outcome (MNIST digits with CNN)\n",
    "\n",
    "| Epochs |  Training Loss |  Training Accuracy |  Validation Accuracy |\n",
    "|--------|----------------|--------------------|----------------------|\n",
    "| 3 |  ~0.35 |  ~88% |  ~87% |\n",
    "| 10 |  ~0.15 |  ~95% |  ~94% |\n",
    "| 30 |  ~0.05 |  ~99% |  ~92% (overfitting begins)\n",
    "\n",
    "* More epochs = better learning initially.\n",
    "* Too many epochs = diminishing returns + overfitting risk.\n",
    "* The sweet spot is usually found by monitoring validation loss/accuracy and using techniques like early stopping or dropout layers."
   ],
   "id": "bbd88ad9145d996"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### üéØ How to Determine if 50 Epochs is Excessive\n",
    "##### 1. Monitor Training vs. Validation Loss\n",
    "* If training loss keeps decreasing but validation loss plateaus or increases, that‚Äôs a strong sign of overfitting.\n",
    "    * Example pattern:\n",
    "        * Epoch 10: Training loss ‚Üì, Validation loss ‚Üì\n",
    "        * Epoch 30: Training loss ‚Üì, Validation loss stable\n",
    "        * Epoch 50: Training loss ‚Üì, Validation loss ‚Üë ‚Üí overfitting.\n",
    "\n",
    "##### 2. Check Accuracy Trends\n",
    "* Training accuracy may approach 99‚Äì100%.\n",
    "* Validation accuracy may peak earlier (say at epoch 20‚Äì30) and then stagnate or decline.\n",
    "* If validation accuracy drops while training accuracy rises, you‚Äôre overfitting.\n",
    "\n",
    "##### 3. Use Early Stopping\n",
    "* Instead of fixing 50 epochs, train with a maximum (like 50) but stop automatically when validation loss stops improving for several epochs.\n",
    "* This prevents wasting time and avoids overfitting.\n",
    "\n",
    "##### 4. Regularization Helps\n",
    "* Techniques like dropout, weight decay (L2 regularization), and data augmentation can allow more epochs without severe overfitting.\n",
    "* But even with these, monitoring validation metrics is essen"
   ],
   "id": "88d2941083391a51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### üîÆ Assumed Outcome for 50 Epochs (CNN on MNIST-like data)\n",
    "\n",
    "| Epochs |  Training Loss |  Training Accuracy |  Validation Accuracy |\n",
    "|--------|----------------|--------------------|----------------------|\n",
    "| 10 |  ~0.15 |  ~95% |  ~94% |\n",
    "| 30 |  ~0.07 |  ~98% |  ~94% (plateau) |\n",
    "| 50 |  ~0.03 |  ~99% |  ~92% (overfitting) |\n",
    "\n",
    "üëâ So yes, 50 epochs can be excessive unless you use early stopping or strong regularization."
   ],
   "id": "1eb38b94ef77bd74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Define a simple CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(10*26*26, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.view(-1, 10*26*26)\n",
    "        x = self.fc1(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# 2. Load dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(torch_data_dir, train=True, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(torch_data_dir, train=False, download=True, transform=transform),\n",
    "    batch_size=64, shuffle=False\n",
    ")\n",
    "\n",
    "# 3. Initialize model, optimizer, and loss function\n",
    "model = SimpleCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 4. Training loop with metrics tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# 5. Plot training vs validation loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', marker='o', markersize=3)\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='s', markersize=3)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss (Overfitting Detection)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../generated/images/50_epochs_loss.png')"
   ],
   "id": "b35845ee0294c01e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Backpropagation in CNNs\n",
    "* Backpropagation is the algorithm used to train CNNs by minimizing the loss function."
   ],
   "id": "85c4be4f7bde85ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Backpropagation is the process of adjusting weights and biases in a neural network by propagating the error (loss) backward through the layers using calculus (derivatives) to minimize that loss.\n",
    "\n",
    "* **Efficiency**: Instead of guessing weight changes, it uses calculus to find the best direction.\n",
    "* **Scalability**: Works for deep networks with many layers.\n",
    "* **Foundation**: It‚Äôs the core algorithm behind training CNNs, RNNs, and modern deep learning models."
   ],
   "id": "e383432fc2497f8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üß† Think of backpropagation like learning to throw darts:\n",
    "\n",
    "* Each throw (forward pass) gives you feedback (loss).\n",
    "* You measure how far off you are (error).\n",
    "* You adjust your aim slightly in the opposite direction of the error (gradient update).\n",
    "* With enough practice (epochs), your throws land closer to the bullseye (minimized loss)."
   ],
   "id": "766d460d0c834944"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üîç Explanation Based on Microsoft Learn Module\n",
    "\n",
    "The Microsoft Learn page on Deep Neural Network Concepts explains backpropagation as part of the training process:\n",
    "\n",
    "###### 1. Forward Pass\n",
    "\n",
    "* Input features (like penguin measurements in the example) are passed through the network.\n",
    "* Each neuron applies its function, producing outputs layer by layer until the final prediction is made.\n",
    "\n",
    "###### 2. Loss Calculation\n",
    "\n",
    "* The prediction is compared to the true label.\n",
    "* The difference (error) is quantified using a loss function (e.g., mean squared error, cross-entropy).\n",
    "* Example: True label `[1,0,0]` vs. predicted `[0.4,0.3,0.3]` ‚Üí variance `[0.6,0.3,0.3]` ‚Üí average loss ~0.18.\n",
    "\n",
    "###### 3. Backpropagation of Error\n",
    "\n",
    "* The entire network can be seen as a nested function.\n",
    "* Using differential calculus, the derivative (gradient) of the loss with respect to each weight and bias is computed.\n",
    "* This tells us whether increasing or decreasing a weight will reduce the loss.\n",
    "* Gradients are propagated backward from the output layer to earlier layers.\n",
    "\n",
    "###### 4. Weight & Bias Updates (Optimization)\n",
    "\n",
    "* An optimizer (like Stochastic Gradient Descent, Adam, or AdaDelta) uses the gradients to adjust weights and biases.\n",
    "* Update rule (simplified):\n",
    "  $$w_{new} = w_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial w}$$\n",
    "  where $\\eta$  is the learning rate.\n",
    "\n",
    "* Small learning rate ‚Üí slow but stable convergence.\n",
    "* Large learning rate ‚Üí faster but risk overshooting the minimum.\n",
    "\n",
    "###### 5. Repeat Across Epochs\n",
    "\n",
    "* Each epoch reuses updated weights and biases.\n",
    "* Over time, the network learns to minimize loss and improve accuracy."
   ],
   "id": "6cff52c3def74176"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üßë‚Äçüíª Simple Backpropagation Example (1 Hidden Layer Neural Network)\n",
    "We‚Äôll train a tiny network to learn the XOR function:"
   ],
   "id": "d463ad393b65ce15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Input data (XOR truth table)\n",
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])   # inputs\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])     # expected outputs\n",
    "\n",
    "# 2. Initialize weights & biases randomly\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(2, 2)   # weights for input -> hidden\n",
    "b1 = np.zeros((1, 2))        # biases for hidden\n",
    "W2 = np.random.randn(2, 1)   # weights for hidden -> output\n",
    "b2 = np.zeros((1, 1))        # biases for output\n",
    "\n",
    "# 3. Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# 4. Training loop\n",
    "lr = 0.1   # learning rate\n",
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---- Forward pass ----\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)              # hidden layer output\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)              # final prediction\n",
    "\n",
    "    # ---- Loss (Mean Squared Error) ----\n",
    "    loss = np.mean((y - a2) ** 2)\n",
    "\n",
    "    # ---- Backpropagation ----\n",
    "    # Output layer error\n",
    "    d_loss_a2 = -(y - a2)         # derivative of loss wrt a2\n",
    "    d_a2_z2 = sigmoid_derivative(z2)\n",
    "    d_z2 = d_loss_a2 * d_a2_z2    # gradient at output\n",
    "\n",
    "    # Gradients for W2 and b2\n",
    "    dW2 = np.dot(a1.T, d_z2)\n",
    "    db2 = np.sum(d_z2, axis=0, keepdims=True)\n",
    "\n",
    "    # Hidden layer error\n",
    "    d_a1_z1 = sigmoid_derivative(z1)\n",
    "    d_z1 = np.dot(d_z2, W2.T) * d_a1_z1\n",
    "\n",
    "    # Gradients for W1 and b1\n",
    "    dW1 = np.dot(X.T, d_z1)\n",
    "    db1 = np.sum(d_z1, axis=0, keepdims=True)\n",
    "\n",
    "    # ---- Update weights & biases ----\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "    # Print progress every 1000 epochs\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ],
   "id": "c3baab9309168622",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### üîç Explanation of the Code:\n",
    "1. Forward pass:\n",
    "   * Compute hidden layer activations (a1) and final predictions (a2).\n",
    "\n",
    "2. Loss calculation:\n",
    "   * Compare predictions with true labels using mean squared error.\n",
    "\n",
    "3. Backward pass (backpropagation):\n",
    "   * Compute gradients of loss w.r.t. output, then propagate back to hidden layer.\n",
    "   * Use chain rule to calculate derivatives for each weight and bias.\n",
    "\n",
    "4. Update step:\n",
    "   * Adjust weights and biases using gradient descent."
   ],
   "id": "b44aa3c85df464d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üìä Expected Output (simplified)\n",
    "```\n",
    "Epoch 0, Loss: 0.2500\n",
    "Epoch 1000, Loss: 0.1253\n",
    "Epoch 2000, Loss: 0.0621\n",
    "Epoch 3000, Loss: 0.0305\n",
    "Epoch 4000, Loss: 0.0152\n",
    "```\n",
    "By the end, the network learns the XOR mapping ‚Äî predictions will be close to `[0,1,1,0]`."
   ],
   "id": "3884cebc3d06080"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Generating XOR backpropagation training with loss visualization over 5000 epochs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sigmoid activation and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# XOR input and output\n",
    "X = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize weights and biases\n",
    "input_layer_neurons = 2\n",
    "hidden_layer_neurons = 2\n",
    "output_neurons = 1\n",
    "\n",
    "# Weights and biases\n",
    "wh = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
    "bh = np.random.uniform(size=(1, hidden_layer_neurons))\n",
    "wo = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
    "bo = np.random.uniform(size=(1, output_neurons))\n",
    "\n",
    "# Training parameters\n",
    "epochs = 5000\n",
    "learning_rate = 0.1\n",
    "loss_history = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    hidden_input = np.dot(X, wh) + bh\n",
    "    hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "    final_input = np.dot(hidden_output, wo) + bo\n",
    "    output = sigmoid(final_input)\n",
    "\n",
    "    # Compute loss (mean squared error)\n",
    "    loss = np.mean((y - output) ** 2)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # Backpropagation\n",
    "    error = y - output\n",
    "    d_output = error * sigmoid_derivative(output)\n",
    "\n",
    "    error_hidden = d_output.dot(wo.T)\n",
    "    d_hidden = error_hidden * sigmoid_derivative(hidden_output)\n",
    "\n",
    "    # Update weights and biases\n",
    "    wo += hidden_output.T.dot(d_output) * learning_rate\n",
    "    bo += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    wh += X.T.dot(d_hidden) * learning_rate\n",
    "    bh += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "# Plotting the loss over epochs\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history, color='blue', linewidth=2)\n",
    "plt.title('XOR Training Loss over Epochs', fontsize=16)\n",
    "plt.xlabel('Epoch', fontsize=14)\n",
    "plt.ylabel('Mean Squared Error Loss', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../generated/images/xor_loss_plot.png')\n",
    "\n",
    "print(\"Trained XOR neural network using backpropagation over 5000 epochs and saved loss plot as xor_loss_plot.png\")\n"
   ],
   "id": "ed5467fb45f0edda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###### üìä What the Chart Shows\n",
    "* Early epochs: Loss starts relatively high because weights are random.\n",
    "* Middle epochs: Loss decreases quickly as the network learns the XOR mapping.\n",
    "* Later epochs: Loss flattens out, showing convergence ‚Äî the network has effectively learned the function."
   ],
   "id": "44334c199b08e8fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üîç Key Takeaways\n",
    "* Backpropagation works by iteratively reducing loss through weight and bias updates.\n",
    "* The curve‚Äôs downward trend confirms the network is learning.\n",
    "* Plateauing at the end means additional epochs won‚Äôt improve much ‚Äî the model has converged.\n",
    "\n",
    "This kind of visualization is exactly how you detect overfitting vs. convergence in larger CNNs: if validation loss starts rising while training loss keeps dropping, that‚Äôs overfitting."
   ],
   "id": "bfeb70dc7ea02fe1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üßë‚Äçüíª Python Simulation: Effect of Learning Rate\n",
    "We‚Äôll train a tiny neural network (same XOR example as before) but run it with different learning rates and compare the loss curves."
   ],
   "id": "e4088448749bfa60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# XOR dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Activation functions\n",
    "def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "def sigmoid_derivative(x): return sigmoid(x)*(1-sigmoid(x))\n",
    "\n",
    "# Training function\n",
    "def train(lr, epochs=2000):\n",
    "    np.random.seed(42)\n",
    "    W1 = np.random.randn(2,2)\n",
    "    b1 = np.zeros((1,2))\n",
    "    W2 = np.random.randn(2,1)\n",
    "    b2 = np.zeros((1,1))\n",
    "    losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        z1 = np.dot(X, W1) + b1\n",
    "        a1 = sigmoid(z1)\n",
    "        z2 = np.dot(a1, W2) + b2\n",
    "        a2 = sigmoid(z2)\n",
    "\n",
    "        # Loss\n",
    "        loss = np.mean((y - a2)**2)\n",
    "        losses.append(loss)\n",
    "\n",
    "        # Backpropagation\n",
    "        d_loss_a2 = -(y - a2)\n",
    "        d_a2_z2 = sigmoid_derivative(z2)\n",
    "        d_z2 = d_loss_a2 * d_a2_z2\n",
    "        dW2 = np.dot(a1.T, d_z2)\n",
    "        db2 = np.sum(d_z2, axis=0, keepdims=True)\n",
    "\n",
    "        d_a1_z1 = sigmoid_derivative(z1)\n",
    "        d_z1 = np.dot(d_z2, W2.T) * d_a1_z1\n",
    "        dW1 = np.dot(X.T, d_z1)\n",
    "        db1 = np.sum(d_z1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= lr * dW1\n",
    "        b1 -= lr * db1\n",
    "        W2 -= lr * dW2\n",
    "        b2 -= lr * db2\n",
    "\n",
    "    return losses\n",
    "\n",
    "# Compare different learning rates\n",
    "lrs = [0.01, 0.1, 1.0]\n",
    "results = {lr: train(lr) for lr in lrs}\n",
    "\n",
    "# Plot\n",
    "for lr, losses in results.items():\n",
    "    plt.plot(losses, label=f\"lr={lr}\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Effect of Learning Rate on Training\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../generated/images/changing_learning_rate.png')\n",
    "\n"
   ],
   "id": "8df4555c25ba7429",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üìä What You‚Äôll See\n",
    "* Low learning rate (0.01): Loss decreases very slowly ‚Äî training is stable but inefficient.\n",
    "* Moderate learning rate (0.1): Loss decreases steadily and converges well ‚Äî often the sweet spot.\n",
    "\n",
    "High learning rate (1.0): Loss may oscillate or diverge ‚Äî updates are too aggressive, overshooting the minimum."
   ],
   "id": "bf4f07bd8f9863ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üß† Key Insight\n",
    "* Too small ‚Üí slow learning.\n",
    "* Too large ‚Üí unstable learning.\n",
    "* Just right ‚Üí fast convergence without overshooting."
   ],
   "id": "5077deb913d7231a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üìù Optimizers\n",
    "> ‚ÄúWe use an optimizer to apply this same trick for all of the weight and bias variables in the model and determine in which direction we need to adjust them (up or down) to reduce the overall amount of loss in the model.‚Äù\n",
    "\n",
    "Optimizers are the engine of backpropagation: they translate gradient information into actual weight and bias updates. Choosing the right optimizer (and learning rate) directly impacts how fast and how well a neural network learns.\n",
    "\n",
    "* **Context**: After calculating the loss and its derivative (gradient), the next step is to decide how to adjust weights and biases to minimize loss.\n",
    "* **Optimizers Defined**: An optimizer is the algorithm that uses gradient information to update weights and biases in the right direction.\n",
    "* **How It Works**:\n",
    "    * The entire neural network can be seen as a nested function.\n",
    "    * By applying differential calculus, we calculate the slope (gradient) of the loss with respect to each weight/bias.\n",
    "    * The optimizer then adjusts parameters up or down depending on whether the gradient is positive or negative.\n",
    "* **Common Optimizers**:\n",
    "    * Stochastic Gradient Descent (SGD): Updates weights using gradients from mini-batches of data.\n",
    "    * AdaDelta: Adapts learning rates dynamically based on recent gradient history.\n",
    "    * Adam (Adaptive Moment Estimation): Combines momentum and adaptive learning rates for faster, more stable convergence.\n",
    "* **Purpose**: All optimizers aim to minimize loss efficiently by finding the best path through the parameter space."
   ],
   "id": "5ecbcd63f24531b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Example: Comparing Optimizers on XOR Problem\n",
    "To trained a small neural network on the XOR dataset using SGD and Adam optimizers, each with learning rates of 0.1 and 0.01. The chart below shows how the loss decreases over 1000 epochs"
   ],
   "id": "9a3b99900774b878"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Simulating training of a simple neural network on XOR data using different optimizers and learning rates\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"../../generated/images\", exist_ok=True)\n",
    "\n",
    "# Define XOR dataset\n",
    "X = torch.tensor([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define a simple neural network\n",
    "class XORNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)\n",
    "        self.fc2 = nn.Linear(4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(optimizer_name, learning_rate, epochs=1000):\n",
    "    model = XORNet()\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if optimizer_name == 'SGD':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    return losses\n",
    "\n",
    "# Settings\n",
    "optimizers = ['SGD', 'Adam']\n",
    "learning_rates = [0.1, 0.01]\n",
    "epochs = 1000\n",
    "\n",
    "# Train models and collect losses\n",
    "results = {}\n",
    "for opt in optimizers:\n",
    "    for lr in learning_rates:\n",
    "        key = f\"{opt}_lr{lr}\"\n",
    "        losses = train_model(opt, lr, epochs)\n",
    "        results[key] = losses\n",
    "\n",
    "# Plotting loss curves\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.figure(figsize=(10, 6))\n",
    "for label, losses in results.items():\n",
    "    plt.plot(losses, label=label)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"XOR Training Loss Comparison: Optimizers and Learning Rates\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plot_path = \"../../generated/images/xor_optimizer_comparison.png\"\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path)\n",
    "\n",
    "print(\"Trained XOR neural network using SGD and Adam optimizers with different learning rates. Loss curves saved as xor_optimizer_comparison.png\")\n"
   ],
   "id": "4ae77dbffff922b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üîç Key Insights\n",
    "* **SGD (lr=0.1)**: Loss decreases steadily but more slowly compared to Adam.\n",
    "* **SGD (lr=0.01)**: Very stable, but convergence is slow ‚Äî the network takes longer to learn.\n",
    "* **Adam (lr=0.1)**: Fast convergence, loss drops quickly, but can oscillate if the rate is too high.\n",
    "* **Adam (lr=0.01)**: Smooth and efficient convergence, often the most balanced choice."
   ],
   "id": "fd703375ed6f151b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üìä Why This Matters\n",
    "* **Optimizers** (SGD, Adam, AdaDelta, etc.) determine how weights and biases are updated during backpropagation.\n",
    "* **Learning rate** controls how much they change each step:\n",
    "    * Too small ‚Üí slow learning.\n",
    "    * Too large ‚Üí unstable or diverging.\n",
    "    * Just right ‚Üí fast and stable convergence."
   ],
   "id": "4451cb298e64723"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### üß† Practical Takeaway\n",
    "* Start with **Adam + lr=0.001‚Äì0.01** for most problems.\n",
    "* Use **SGD** when you want more control or are fine-tuning.\n",
    "* Always monitor **validation loss** to avoid overfitting or divergence."
   ],
   "id": "feea6b2610416171"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

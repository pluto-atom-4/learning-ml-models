{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with regression models\n",
    "\n",
    "We'll experiment with more complex models to improve our regression performance.\n"
   ],
   "id": "5321170d57ce84ae"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import modules we'll need for this notebook\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the training dataset\n",
    "bike_data = pd.read_csv('../../generated/data/raw/daily-bike-share.csv')\n",
    "bike_data['day'] = pd.DatetimeIndex(bike_data['dteday']).day\n",
    "numeric_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "categorical_features = ['season','mnth','holiday','weekday','workingday','weathersit', 'day']\n",
    "bike_data[numeric_features + ['rentals']].describe()\n",
    "print(bike_data.head())\n",
    "\n",
    "\n",
    "# Separate features and labels\n",
    "# After separating the dataset, we now have numpy arrays named **X** containing the features, and **y** containing the labels.\n",
    "X, y = bike_data[['season','mnth', 'holiday','weekday','workingday','weathersit','temp', 'atemp', 'hum', 'windspeed']].values, bike_data['rentals'].values\n",
    "\n",
    "# Split data 70%-30% into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "print ('Training Set: %d rows\\nTest Set: %d rows' % (X_train.shape[0], X_test.shape[0]))\n"
   ],
   "id": "be2788c672fc75a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we have the following four datasets:\n",
    "\n",
    "- **X_train**: The feature values we'll use to train the model\n",
    "- **y_train**: The corresponding labels we'll use to train the model\n",
    "- **X_test**: The feature values we'll use to validate the model\n",
    "- **y_test**: The corresponding labels we'll use to validate the model\n",
    "\n",
    "Now we're ready to train a model by fitting a suitable regression algorithm to the training data.\n",
    "\n",
    "## Experiment with Algorithms\n",
    "\n",
    "The linear-regression algorithm we used last time to train the model has some predictive capability, but there are many kinds of regression algorithm we could try, including:\n",
    "\n",
    "- **Linear algorithms**: Not just the Linear Regression algorithm we used above (which is technically an *Ordinary Least Squares* algorithm), but other variants such as *Lasso* and *Ridge*.\n",
    "- **Tree-based algorithms**: Algorithms that build a decision tree to reach a prediction.\n",
    "- **Ensemble algorithms**: Algorithms that combine the outputs of multiple base algorithms to improve generalizability.\n",
    "\n",
    "> **Note**: For a full list of Scikit-Learn estimators that encapsulate algorithms for supervised machine learning, see the [Scikit-Learn documentation](https://scikit-learn.org/stable/supervised_learning.html). There are many algorithms from which to choose, but for most real-world scenarios, the [Scikit-Learn estimator cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html) can help you find a suitable starting point.\n",
    "\n",
    "**Try Another Linear Algorithm**\n",
    "\n",
    "Let's try training our regression model by using a **Lasso** algorithm. We can do this by just changing the estimator in the training code."
   ],
   "id": "333bd45b0cbb0f6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Fit a lasso model on the training set\n",
    "model = Lasso().fit(X_train, y_train)\n",
    "print (model, \"\\n\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.scatter(y_test, predictions, label='Predictions')\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Daily Bike Share Predictions')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test,p(y_test), color='magenta', label='Regression Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "595f3a1e4ce7f908",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Try a Decision Tree Algorithm\n",
    "\n",
    "As an alternative to a linear model, there's a category of algorithms for machine learning that uses a tree-based approach in which the features in the dataset are examined in a series of evaluations, each of which results in a *branch* in a *decision tree* based on the feature value. At the end of each series of branches are leaf-nodes with the predicted label value based on the feature values.\n",
    "\n",
    "It's easiest to see how this works with an example. Let's train a Decision Tree regression model using the bike rental data. After training the model, the following code will print the model definition and a text representation of the tree it uses to predict label values."
   ],
   "id": "d6c8f6cd7ef82f77"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_text\n",
    "\n",
    "# Train the model\n",
    "model = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "print (model, \"\\n\")\n",
    "\n",
    "# Visualize the model tree\n",
    "tree = export_text(model)\n",
    "print(tree)"
   ],
   "id": "64c439db8ff9ad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "So now we have a tree-based model, but is it any good? Let's evaluate it with the test data.",
   "id": "cc55a74c16658282"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluate the model using the test data\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.scatter(y_test, predictions, label='Predictions')\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Daily Bike Share Predictions')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test, p(y_test), color='magenta', label='Regression Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f2f957d9ea56d1a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tree-based model doesn't seem to have significantly improved over the linear model, so what else could we try?\n",
    "\n",
    "### Try an Ensemble Algorithm\n",
    "\n",
    "Ensemble algorithms work by combining multiple base estimators to produce an optimal model, either by applying an aggregate function to a collection of base models (sometimes referred to a *bagging*) or by building a sequence of models that build on one another to improve predictive performance (referred to as *boosting*).\n",
    "\n",
    "For example, let's try a Random Forest model, which applies an averaging function to multiple Decision Tree models for a better overall model."
   ],
   "id": "c168fb761bb5a8aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor().fit(X_train, y_train)\n",
    "print (model, \"\\n\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.scatter(y_test, predictions, label='Predictions')\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Daily Bike Share Predictions')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test,p(y_test), color='magenta', label='Regression Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "4dd02f8704ec8187",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For good measure, let's also try a *boosting* ensemble algorithm. We'll use a Gradient Boosting estimator, which like a Random Forest algorithm builds multiple trees; but instead of building them all independently and taking the average result, each tree is built on the outputs of the previous one in an attempt to incrementally reduce the *loss* (error) in the model.",
   "id": "37d1536e7b4dc7c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Fit a lasso model on the training set\n",
    "model = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "print (model, \"\\n\")\n",
    "\n",
    "# Evaluate the model using the test data\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"MSE:\", mse)\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"RMSE:\", rmse)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(\"R2:\", r2)\n",
    "\n",
    "# Plot predicted vs actual\n",
    "plt.scatter(y_test, predictions, label='Predictions')\n",
    "plt.xlabel('Actual Labels')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Daily Bike Share Predictions')\n",
    "# overlay the regression line\n",
    "z = np.polyfit(y_test, predictions, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test,p(y_test), color='magenta', label='Regression Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "786c58098e2a91dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "Here, we've tried a number of new regression algorithms to improve performance."
   ],
   "id": "208ddb68e2ba3575"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Performance Comparison\n",
    "### Lasso & Decision Tree (Identical Results):\n",
    "\n",
    "* MSE: 201,155.71 | RMSE: 448.50 | R² = 0.606\n",
    "* These models perform similarly with moderate accuracy, explaining ~60.6% of variance\n",
    "\n",
    "### Ensemble (Random Forest) (Best Performer):\n",
    "\n",
    "* MSE: 112,532.26 | RMSE: 335.46 | R² = 0.779\n",
    "* Significantly outperforms with ~78% variance explained\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "| Metric         | Improvement                  |\n",
    "|----------------|------------------------------|\n",
    "| MSE Reduction  | 44% lower (112.5K vs 201.2K) |\n",
    "| RMSE Reduction | 25% lower (335.5 vs 448.5)   |\n",
    "| R² Score       | +17.4 percentage points      |\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "The **Ensemble model** is clearly superior because:\n",
    "\n",
    "1. Lower prediction error - RMSE of 335.5 means average prediction error is ~335 units vs 448.5\n",
    "2. Better variance explanation - Captures 78% of data patterns vs 61%\n",
    "3. Reduced overfitting risk - Ensemble methods (Random Forest) combine multiple trees to stabilize predictions\n",
    "4. Robustness - Handles non-linear relationships better than Lasso (linear) or single Decision Trees\n",
    "\n",
    "The identical Lasso/Decision Tree results suggest potential issues (verify data preprocessing or hyperparameters). The Ensemble's superior performance validates using ensemble methods for this dataset."
   ],
   "id": "727b24d5f80db191"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize classification models\n",
    "\n",
    "To look at the alternatives to accuracy that can be much more useful in machine learning.\n"
   ],
   "id": "3174d482a9c1eea6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Alternative metrics for binary classifiers\n",
    "* **Accuracy** can be misleading with class imbalance.\n",
    "  Example: if 3% are diabetic, always predicting \"no\" gives 97% accuracy but fails to detect diabetics.\n",
    "* Prefer metrics that capture error types: precision, recall, F1, and the confusion matrix.\n",
    "* Scikit-Learn's classification_report and confusion_matrix summarize these metrics for better insight."
   ],
   "id": "7ad9aca74d89311f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# load the training dataset\n",
    "diabetes = pd.read_csv('../../generated/data/raw/diabetes.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "features = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']\n",
    "label = 'Diabetic'\n",
    "X, y = diabetes[features].values, diabetes[label].values\n",
    "\n",
    "\n",
    "# Split data 70%-30% into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "print ('Training cases: %d\\nTest cases: %d' % (X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "# Train the model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set regularization rate\n",
    "reg = 0.01\n",
    "\n",
    "# train a logistic regression model on the training set\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print('Predicted labels: ', predictions)\n",
    "print('Actual labels:    ', y_test)\n",
    "\n",
    "\n",
    "print('Accuracy: ', accuracy_score(y_test, predictions))\n"
   ],
   "id": "25ead7cd0829446",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lambda_vals = [1.0, 0.1, 0.01, 0.001]\n",
    "\n",
    "Cs = [1.0 / l for l in lambda_vals]\n",
    "coef_norms = []\n",
    "\n",
    "for l, C in zip(lambda_vals, Cs):\n",
    "    model = LogisticRegression(C=C, solver='liblinear', max_iter=1000).fit(X_train, y_train)\n",
    "    coef_norms.append(np.linalg.norm(model.coef_))\n",
    "    print(f\"lambda={l:.3g} -> C={C:.3g}, coef_norm={coef_norms[-1]:.3f}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambda_vals, coef_norms, marker='o')\n",
    "plt.xscale('log')\n",
    "plt.gca().invert_xaxis()  # optional: show stronger regularization (larger lambda) on left\n",
    "plt.xlabel('lambda (regularization rate)')\n",
    "plt.ylabel('||coef|| (L2 norm)')\n",
    "plt.title('Coefficient norm vs lambda (log scale)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "24b6d0a1ecbe2be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* The list [1.0, 0.1, 0.01, 0.001] are values for the regularization rate ($\\lambda$) (lambda).\n",
    "* scikit-learn's `LogisticRegression` uses `C = 1 / lambda`. So:\n",
    "  - Larger ($\\lambda$) -> smaller `C` -> stronger regularization (more shrinkage toward zero).\n",
    "  - Smaller ($\\lambda$) -> larger `C` -> weaker regularization (less shrinkage, larger coefficients).\n",
    "\n",
    "* That particular list spans three orders of magnitude (decades): it performs a *log-scale* sweep from stronger to weaker regularization (1 â†’ 0.001).\n",
    "* Typical effect: as lambda decreases (C increases) coefficient norms increase and risk of overfitting grows; as lambda increases coefficients shrink and the model can underfit.\n",
    "* Practical guidance: search ($\\lambda$) on a log scale (e.g. from 1e-4 to 1e2) and pick the best value by cross-validation (e.g. `GridSearchCV` or `LogisticRegressionCV`).\n",
    "\n",
    "Brief demo (assumes `X_train` and `y_train` exist): the code below maps the lambda values to `C`, fits models, prints C and coefficient norms, and plots coefficient norm vs lambda.\n"
   ],
   "id": "d334298f9d4ec852"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Regularization penalizes large model weights **to reduce variance and prevent overfitting**; it trades model complexity for smoother, more generalizable decision boundaries.\n",
    "- In scikit-learn's `LogisticRegression`, the parameter `C` controls regularization and equals `1 / lambda`. A bigger `lambda` means stronger regularization (so `C` is smaller); a smaller lambda means weaker regularization (so C is larger).\n",
    "- Effects on training: stronger regularization (large `lambda`, small `C`) shrinks coefficients toward zero, may lower training accuracy but often improves test performance; weaker regularization can increase training accuracy but risks overfitting.\n",
    "- Penalty types: `L2` (ridge) shrinks coefficients smoothly; `L1` (lasso) can set many coefficients exactly to zero (sparse models). Choose based on interpretability and feature selection needs.\n",
    "- Practical notes: always scale features before regularizing (e.g., `StandardScaler`), search `lambda` or `C` on a log scale (e.g., lambda in [1e-4, 1e2] or C in [1e-2, 1e4]) via cross-validation (`GridSearchCV` or `LogisticRegressionCV`), and pick a solver that supports your penalty (`liblinear`, `saga`, etc.).\n",
    "- Numerical/convergence: very weak regularization sometimes slows convergence; increase `max_iter` or switch solver if you see convergence warnings.\n",
    "Brief example: use `LogisticRegressionCV` to tune C (inverse regularization) with scaling and cross-validation."
   ],
   "id": "be4c8c2a638a6669"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pipeline scales features, then finds best C on a log scale via cross-validation.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "clf = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegressionCV(Cs=10**np.linspace(-4, 4, 9), cv=5, penalty='l2', solver='liblinear', max_iter=1000)\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best C:\", clf.named_steps['logisticregressioncv'].C_)"
   ],
   "id": "ee72b6635ab9b8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One of the simplest places to start is a classification report. Run the next cell to see a range of alternate ways to assess our model.",
   "id": "532a53ab0ee25aba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn. metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, predictions))"
   ],
   "id": "330327085cbbc86f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The classification report includes the following metrics for each class (0 and 1):\n",
    "\n",
    "* *Precision*: Of the predictions the model made for this class, what proportion were correct?\n",
    "* *Recall*: Out of all of the instances of this class in the test dataset, how many did the model identify?\n",
    "* *F1-Score*: An average metric that takes both precision and recall into account.\n",
    "* *Support*: How many instances of this class are there in the test dataset?\n",
    "\n",
    "The classification report also includes averages for these metrics, including a weighted average that allows for the imbalance in the number of cases of each class.\n",
    "\n",
    "Because this is a *binary* classification problem, the ***1*** class is considered *positive* and its precision and recall are particularly interesting - these in effect answer the questions:\n",
    "\n",
    "    - Of all the patients the model predicted are diabetic, how many are actually diabetic?\n",
    "    - Of all the patients that are actually diabetic, how many did the model identify?\n",
    "\n"
   ],
   "id": "1800035c9d8c532a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To You can retrieve these values on their own by using the **precision_score** and **recall_score** metrics in Scikit-Learn (which by default assume a binary classification model).",
   "id": "d973a3a3989d3f10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "print(\"Overall Precision:\", precision_score(y_test, predictions))\n",
    "print(\"Overall Recall:\", recall_score(y_test, predictions))"
   ],
   "id": "3af81e5c3fe7a61d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The precision and recall metrics are derived from four possible prediction outcomes:\n",
    "* *True Positives*: The predicted label and the actual label are both 1.\n",
    "* *False Positives*: The predicted label is 1, but the actual label is 0.\n",
    "* *False Negatives*: The predicted label is 0, but the actual label is 1.\n",
    "* *True Negatives*: The predicted label and the actual label are both 0.\n",
    "\n",
    "These metrics are generally tabulated for the test set and shown together as a *confusion matrix*, which takes the following form:\n",
    "\n",
    "<table style=\"border: 1px solid black; width: 50px;margin-left: auto; margin-right: auto;\">\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FP</td>\n",
    "    </tr>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <td style=\"border: 1px solid black;color: black;\" bgcolor=\"white\">FN</td><td style=\"border: 1px solid black;color: black;\" bgcolor=\"lightgray\">TP</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Note that the correct (*true*) predictions form a diagonal line from top left to bottom right - these figures should be significantly higher than the *false* predictions if the model is any good.\n"
   ],
   "id": "6a2199c472808646"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To compute the confusion matrix for your trained classifier using scikit-learn's` sklearn.metrics.confusion_matrix` on the true labels and the model's predictions.",
   "id": "2b2252a8365c5588"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Print the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print (cm)"
   ],
   "id": "63e5931a14214c3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "It's common to plot a confusion matrix as a *heat map* with shade of each cell reflecting its value. In an ideal model, the shading forms a diagonal pattern of darker shades from top-left to bottom-right, indicating higher values where the *predicted* and *actual* labels match.",
   "id": "af8488a7623cbb28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Create the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a figure and axis\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Create a heatmap using matplotlib\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "# Add labels\n",
    "classes = ['No Diabetes', 'Diabetes']\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "# Add text annotations\n",
    "thresh = cm.max() / 2.\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "58dda1b6a554f020",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "* Models often show final outputs as class labels (0 or 1), but they actually produce probabilities.\n",
    "* A binary classifier (e.g., logistic regression) estimates P(y) and 1 - P(y).\n",
    "* A threshold (default 0.5) converts probability to a label.\n",
    "* Decision rule: predict **1** if `P(y) > 0.5`; predict **0** if `P(y) <= 0.5`."
   ],
   "id": "cecb36cf9b72b96f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To call the model's `predict_proba` on `X_test` to inspect the probability pair for each sample (column 0 = P(class 0), column 1 = P(class 1)).\n",
   "id": "91df43660f5625b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_scores = model.predict_proba(X_test)\n",
    "print(y_scores)"
   ],
   "id": "c8ca9309ff6026f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The decision to score a prediction as `1` or `0` depends on the threshold applied to the predicted probability.\n",
    "- Changing the threshold alters the predictions and therefore the values in the confusion matrix.\n",
    "- A common evaluation is to compute the *true positive rate* (TPR, i.e.\\ recall) and the *false positive rate* (FPR) across a range of thresholds.\n",
    "- Examining TPR and FPR over many thresholds (e.g.\\ via the ROC curve) assesses classifier performance independent of a single threshold."
   ],
   "id": "cc977d0c219308fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To plot the true positive rate (TPR) against the false positive rate (FPR) across all decision thresholds to generate the receiver operating characteristic (ROC) curve.",
   "id": "d8c155af12fe11a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# plot ROC curve\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ],
   "id": "7c05bb5af85bea18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The ROC chart plots the true positive rate (TPR) against the false positive rate (FPR) for threshold values from `0` to `1`.\n",
    "- A perfect classifier's curve goes straight up the left axis and then straight across the top.\n",
    "- The diagonal line denotes a 50/50 random classifier; the ROC curve should lie above it to show performance better than guessing.\n",
    "\n",
    "The area under the curve (*AUC*) is a value between 0 and 1 that quantifies the overall performance of the model. The closer to 1 this value is, the better the model."
   ],
   "id": "8f68a6443f411544"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To call `scikit-learn`'s `roc_auc_score` with the true labels and the predicted positive-class scores (e.g. `y_test`, `y_scores[:, 1]`) to compute the area under the ROC curve.",
   "id": "fbe5d1b4c5ccf287"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))"
   ],
   "id": "2981bbaed69c12db",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Perform preprocessing in a pipeline\n",
    "\n",
    "In this case, the ROC curve and its AUC indicate that the model performs better than a random guess which is not bad considering we performed very little preprocessing of the data.\n",
    "\n",
    "In practice, it's common to perform some preprocessing of the data to make it easier for the algorithm to fit a model to it. There's a huge range of preprocessing transformations you can perform to get your data ready for modeling, but we'll limit ourselves to a few common techniques:\n",
    "\n",
    "- Scaling numeric features so they're on the same scale. This prevents features with large values from producing coefficients that disproportionately affect the predictions.\n",
    "- Encoding categorical variables. For example, by using a *one-hot encoding* technique you can create individual binary (true/false) features for each possible category value.\n",
    "\n"
   ],
   "id": "da063035abdbf362"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To apply these preprocessing transformations, we'll make use of a Scikit-Learn feature named *pipelines*. Pipelines enable us to define a set of preprocessing steps that end with an algorithm. You can then apply the entire pipeline to the data, so that the model encapsulates all of the preprocessing steps as well as the regression algorithm. This is useful, because when we want to use the model to predict values from new data, we'll need to apply the same transformations (based on the same statistical distributions and category encodings used with the training data).\n",
    "\n",
    "> **Note**: The term *pipeline* is used extensively in machine learning, often to mean very different things! In this context, we're using it to refer to pipeline objects in Scikit-Learn."
   ],
   "id": "82d11192e655911"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "# Define preprocessing for numeric columns (normalize them so they're on the same scale)\n",
    "numeric_features = [0, 1, 2, 3, 4, 5, 6]\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Define preprocessing for categorical features (encode the Age column)\n",
    "categorical_features = [7]\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Create preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('logregressor', LogisticRegression(C=1 / reg, solver=\"liblinear\"))])\n",
    "\n",
    "# fit the pipeline to train a logistic regression model on the training set\n",
    "model = pipeline.fit(X_train, (y_train))\n",
    "print(model)"
   ],
   "id": "e064671f6d2fd80a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This pipeline encapsulates the preprocessing steps and also model training.",
   "id": "5e4a619f765e172c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To use the pipeline-trained model to predict labels for `X_test` (e.g., `model.predict(X_test)`), compute key metrics (`accuracy`, `precision`, `recall`, `f1`, `confusion_matrix`, `roc_auc_score`), and compare these results to the basic model's metrics to assess improvement.",
   "id": "9648013f385127a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get predictions from test data\n",
    "predictions = model.predict(X_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "\n",
    "# Get evaluation metrics\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print ('Confusion Matrix:\\n',cm, '\\n')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))\n",
    "print(\"Overall Precision:\", precision_score(y_test, predictions))\n",
    "print(\"Overall Recall:\", recall_score(y_test, predictions))\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "\n",
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# plot ROC curve\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "93c2e2a1f88a3eb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The results do look a little better, so clearly preprocessing the data has made a difference.",
   "id": "544f6c59d3d1cacf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To try a different algorithm and evaluate it against the logistic regression baseline.\n",
    "\n",
    "- Choose alternative algorithms:\n",
    "  - Support Vector Machine (SVM)\n",
    "    - Select `sklearn.svm.SVC` with `probability=True`.\n",
    "    - Scale features (`StandardScaler`) before fitting.\n",
    "    - Fit on `X_train`, `y_train`.\n",
    "    - Tune hyperparameters (`C`, `kernel`, `gamma`) with `GridSearchCV` or `RandomizedSearchCV`.\n",
    "    - Compute `predict`, `predict_proba`, and evaluation metrics.\n",
    "  - Tree-based algorithms\n",
    "    - Select `sklearn.tree.DecisionTreeClassifier` or `sklearn.ensemble.RandomForestClassifier`.\n",
    "    - No mandatory scaling; optionally try feature scaling for consistency.\n",
    "    - Fit on `X_train`, `y_train`.\n",
    "    - Tune hyperparameters (`max_depth`, `min_samples_leaf`, `n_estimators`) with cross-validation.\n",
    "    - Compute predictions and metrics.\n",
    "  - Ensemble algorithms\n",
    "    - Try `RandomForestClassifier`, `GradientBoostingClassifier`, or `xgboost.XGBClassifier`.\n",
    "    - Fit and tune (`n_estimators`, `learning_rate`, `max_depth`) via CV.\n",
    "    - Evaluate as below.\n",
    "\n",
    "- For each chosen algorithm:\n",
    "  - Build a pipeline: `Pipeline([('scaler', StandardScaler()), ('clf', <estimator>)])` (omit scaler for pure tree methods if desired).\n",
    "  - Fit the pipeline on `X_train`, `y_train`.\n",
    "  - Obtain `predictions = model.predict(X_test)` and `y_scores = model.predict_proba(X_test)[:, 1]` (or decision scores).\n",
    "  - Compute metrics: `accuracy`, `precision`, `recall`, `f1`, `confusion_matrix`, `roc_auc_score`.\n",
    "  - Plot confusion matrix and ROC curve (`roc_curve`) to visualize performance across thresholds.\n",
    "  - Record cross-validated scores and best hyperparameters.\n",
    "\n",
    "- Compare and select:\n",
    "  - Compare each model's metrics and ROC AUC to the baseline logistic regression.\n",
    "  - Prefer models with higher `roc_auc_score` and better precision/recall trade-off for the positive class.\n",
    "  - If multiple models perform similarly, prefer simpler or more interpretable models.\n",
    "\n",
    "- Finalize:\n",
    "  - Choose the best model, re-fit on full training data if needed.\n",
    "  - Persist the pipeline using `joblib.dump` for later inference."
   ],
   "id": "2bea94eb9a477dba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In order to train the model using an *ensemble* algorithm named *Random Forest* that combines the outputs of multiple random decision trees. For more details, see the [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/ensemble.html#forests-of-randomized-trees).",
   "id": "3f0e0e117562bdb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create preprocessing and training pipeline\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('logregressor', RandomForestClassifier(n_estimators=100))])\n",
    "\n",
    "# fit the pipeline to train a random forest model on the training set\n",
    "model = pipeline.fit(X_train, (y_train))\n",
    "print (model)"
   ],
   "id": "84737c4c409b4a8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To look at the performance metrics for the new model.",
   "id": "6c3729589543d285"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "predictions = model.predict(X_test)\n",
    "y_scores = model.predict_proba(X_test)\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "print ('Confusion Matrix:\\n',cm, '\\n')\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))\n",
    "print(\"Overall Precision:\",precision_score(y_test, predictions))\n",
    "print(\"Overall Recall:\",recall_score(y_test, predictions))\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('\\nAUC: ' + str(auc))\n",
    "\n",
    "# calculate ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
    "\n",
    "# plot ROC curve\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "# Plot the diagonal 50% line\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "# Plot the FPR and TPR achieved by our model\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.show()"
   ],
   "id": "96813996c13abd16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "That looks better!",
   "id": "861071fb81623bb5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To save the trained pipeline/model for later use (e.g. `joblib.dump(model, 'model.joblib')`), then load it when needed with `joblib.load('model.joblib')` and call model.predict(X_new) or model.predict_proba(X_new) to infer labels for new data, ensuring X_new uses the same preprocessing and feature order as the training data.",
   "id": "d98565d79d7fd682"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Save the trained pipeline/model for later\n",
    "  - Use `joblib.dump(model, \\`model.joblib\\`)`.\n",
    "  - Store alongside metadata (training date, data version).\n",
    "\n",
    "- Load the saved model when needed\n",
    "  - Use `joblib.load(\\`model.joblib\\`)` to restore the pipeline.\n",
    "  - Inspect with `loaded.named_steps` to confirm preprocessing and estimator.\n",
    "\n",
    "- Infer on new data\n",
    "  - Call `model.predict(X_new)` for labels or `model.predict_proba(X_new)` for probabilities.\n",
    "  - If the pipeline includes preprocessing, pass raw inputs; otherwise apply the same preprocessing before predicting.\n",
    "\n",
    "- Ensure consistency\n",
    "  - `X_new` must have the same feature order, types, and expected columns as the training data.\n",
    "  - Prefer using the saved pipeline so encoders/scalers match the training transforms."
   ],
   "id": "c015b82e2cae3a50"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model as a pickle file\n",
    "filename = '../../generated/models/diabetes_model.pkl'\n",
    "joblib.dump(model, filename)"
   ],
   "id": "7688da6813672b95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next step: load the trained pipeline/model and use it to predict labels for new observations.\n",
    "\n",
    "- Load the saved model\n",
    "  - Run `import joblib`\n",
    "  - Restore the pipeline using `joblib.load` with file `model.joblib`\n",
    "\n",
    "- Prepare new data\n",
    "  - Ensure `X_new` has the same feature columns, order, types, and missing-value handling as the training data\n",
    "  - If the pipeline contains preprocessing, pass raw inputs; otherwise apply the same preprocessing transforms\n",
    "\n",
    "- Predict\n",
    "  - Compute labels: `preds = model.predict(X_new)`\n",
    "  - Compute probabilities: `probs = model.predict_proba(X_new)[:, 1]`\n",
    "  - Optionally apply a custom threshold to `probs` to derive final labels\n",
    "\n",
    "- Validate and monitor\n",
    "  - Spot-check predictions against any available ground truth\n",
    "  - Track input distribution and prediction drift over time\n",
    "\n",
    "- Save and document results\n",
    "  - Export predictions with identifiers to file `predictions.csv`\n",
    "  - Record model version, training date, and data version alongside results"
   ],
   "id": "3d87fd020e6f3599"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the model from the file\n",
    "model = joblib.load(filename)\n",
    "\n",
    "# predict on a new sample\n",
    "# The model accepts an array of feature arrays (so you can predict the classes of multiple patients in a single call)\n",
    "# We'll create an array with a single array of features, representing one patient\n",
    "X_new = np.array([[2,180,74,24,21,23.9091702,1.488172308,22]])\n",
    "print ('New sample: {}'.format(list(X_new[0])))\n",
    "\n",
    "# Get a prediction\n",
    "pred = model.predict(X_new)\n",
    "\n",
    "# The model returns an array of predictions - one for each set of features submitted\n",
    "# In our case, we only submitted one patient, so our prediction is the first one in the resulting array.\n",
    "print('Predicted class is {}'.format(pred[0]))"
   ],
   "id": "9ae8ee42b050d5e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we looked at a range of metrics for binary classification and tried a few algorithms beyond logistic regression. We'll move onto more complex classification problems in the next notebook."
   ],
   "id": "2c9001de5ad23b9e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

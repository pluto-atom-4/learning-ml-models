{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Clustering - K-Means and Hierarchical\n",
    "\n",
    "---\n",
    "\n",
    "## Block 1: Use Case Scenarios for Clustering\n",
    "\n",
    "### What is the practical use of clustering?\n",
    "\n",
    "Clustering is used to automatically group data into distinct clusters without knowing how many clusters there are or what they indicate beforehand. Common real-world applications include:\n",
    "\n",
    "**1. Customer Segmentation**\n",
    "- A marketing organization might separate customers into distinct segments\n",
    "- Then investigate how those segments exhibit different purchasing behaviors\n",
    "- Actionable insight: tailor products/services to each segment\n",
    "\n",
    "**2. Clustering as a Preprocessing Step for Classification**\n",
    "- Start by identifying distinct groups of data points using unsupervised learning\n",
    "- Assign class labels to those clusters based on domain knowledge\n",
    "- Use the labeled data to train a supervised classification model\n",
    "- Benefit: reduces human labeling effort and ensures meaningful labels\n",
    "\n",
    "**3. Species/Category Discovery**\n",
    "- In the seeds dataset example: three seed species (0=*Kama*, 1=*Rosa*, 2=*Canadian*) are already known\n",
    "- Clustering can separate seeds by physical characteristics (area, perimeter, compactness, etc.)\n",
    "- Compare the unsupervised cluster assignments to the true species labels\n",
    "- Validate whether clustering algorithms can naturally discover species boundaries\n",
    "\n",
    "---\n",
    "\n",
    "## Block 2: Practical Implementation Strategy - Seeds Dataset Example\n",
    "\n",
    "### Objective\n",
    "Create a K-Means clustering model using the seeds dataset to:\n",
    "1. Automatically group seeds into clusters based on physical features\n",
    "2. Evaluate cluster quality using silhouette score\n",
    "3. Compare clusters to known seed species\n"
   ],
   "id": "cb107c1d93464ea2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from plot_clusters import plot_clusters\n"
   ],
   "id": "6fd319975f7a1d0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load the training dataset\n",
    "data = pd.read_csv('../../generated/data/raw/seeds.csv')\n",
    "\n",
    "# Display a random sample of 10 observations (just the features)\n",
    "features = data[data.columns[0:6]]\n",
    "features.sample(10)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Normalize the numeric features so they're on the same scale\n",
    "scaled_features = MinMaxScaler().fit_transform(features[data.columns[0:6]])\n",
    "\n",
    "# Get two principal components\n",
    "pca = PCA(n_components=2).fit(scaled_features)\n",
    "features_2d = pca.transform(scaled_features)\n",
    "print(features_2d[0:10])"
   ],
   "id": "a7f16f5d232515f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### K-Means Clustering Algorithm Overview\n",
    "\n",
    "**How K-Means Works:**\n",
    "\n",
    "1. A set of K centroids are randomly chosen.\n",
    "2. Clusters are formed by assigning the data points to their closest centroid.\n",
    "3. The mean of each cluster is computed and the centroid is moved to the mean.\n",
    "4. Steps 2 and 3 are repeated until a stopping criteria is met. Typically, the algorithm terminates when each new iteration results in negligible movement of centroids and the clusters become static.\n",
    "5. When the clusters stop changing, the algorithm has *converged*, defining the locations of the clusters. Note that the random starting point for the centroids means that re-running the algorithm could result in slightly different clusters, so training usually involves multiple iterations, re-initializing the centroids each time, and the model with the best WCSS (*within cluster sum of squares*) is selected.\n"
   ],
   "id": "644eaacf5f996f74"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 1: Prepare Data - Feature Extraction and Scaling",
   "id": "5821a4650292b9aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a model based on 3 centroids\n",
    "# For the seeds dataset, we'll use K=3 clusters because we know there are 3 seed species\n",
    "# The algorithm will discover whether the physical features naturally separate into 3 distinct groups\n",
    "\n",
    "model = KMeans(n_clusters=3, init='k-means++', n_init=100, max_iter=1000)\n",
    "\n",
    "# Fit to the data and predict the cluster assignments for each data point\n",
    "km_clusters = model.fit_predict(features.values)\n",
    "\n",
    "# View the cluster assignments\n",
    "print(km_clusters)"
   ],
   "id": "becdb545a446b25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Visualize Cluster Assignments\n",
    "\n",
    "We'll plot the clusters using two principal components to see how well the seeds are separated.\n"
   ],
   "id": "351e36885e89d553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_clusters(features_2d, km_clusters)\n",
   "id": "d46b4253f93ef875",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The data should be separated into three distinct clusters. If not, rerun the previous steps.\n",
   "id": "69768ea60dc9b473"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Evaluate Cluster Quality with Silhouette Score\n",
    "\n",
    "To quantify how well the clusters are separated, we calculate a **silhouette score** - a metric with a value between -1 and +1. The closer this value is to +1, the better separated the clusters are.\n",
    "\n",
    "**Silhouette Score Interpretation:**\n",
    "- **+1**: Points are very close to their own cluster, far from others (excellent separation)\n",
    "- **0**: Points sit on cluster boundaries (ambiguous)\n",
    "- **-1**: Points are in the wrong cluster (poor separation)\n",
    "\n",
    "**How it works**: For each point, compare its distance to points in its own cluster (a) vs. the nearest other cluster (b). Score = (b - a) / max(a, b)\n"
   ],
   "id": "f655694052b0d0fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Compute the silhouette score\n",
    "silhouette_avg = silhouette_score(features, km_clusters)\n",
    "print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "\n",
    "# Interpretation\n",
    "if silhouette_avg > 0.5:\n",
    "    print(\"✓ Strong cluster separation - clusters are well-defined\")\n",
    "elif silhouette_avg > 0.3:\n",
    "    print(\"→ Reasonable cluster structure - acceptable separation\")\n",
    "else:\n",
    "    print(\"✗ Weak or overlapping clusters - consider adjusting k or reviewing data\")\n"
   ],
   "id": "778dc80990f9c83e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So what's the practical use of clustering? In some cases, you'll have data that you need to group into distinct clusters without knowing how many clusters there are or what they indicate. For example, a marketing organization might want to separate customers into distinct segments, and then investigate how those segments exhibit different purchasing behaviors.\n",
    "\n",
    "Sometimes, clustering is used as an initial step towards creating a classification model. You start by identifying distinct groups of data points, and then assign class labels to those clusters. You can then use this labelled data to train a classification model.\n",
    "\n"
   ],
   "id": "a8480bb6005ba3ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In the case of the seeds data, the different species of seed are already known and encoded as 0 (*Kama*), 1 (*Rosa*), or 2 (*Canadian*), so we can use these identifiers to compare the species classifications to the clusters identified by our unsupervised algorithm.",
   "id": "591284c7d0777c71"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "seed_species = data[data.columns[7]]\n",
    "seed_species"
   ],
   "id": "c4558beda228473e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_clusters(features_2d, seed_species.values)",
   "id": "5a0b414bf536bd96",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "There may be some differences between the cluster assignments and class labels, but the K-Means model should have done a reasonable job of clustering the observations so that seeds of the same species are generally in the same cluster.",
   "id": "2a93e1b29816e5ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering methods make fewer distributional assumptions when compared to K-Means methods. However, K-Means methods are generally more scalable, sometimes very much so.\n",
    "\n",
    "Hierarchical clustering creates clusters by using either a *divisive* method or an *agglomerative* method. The divisive method is a \"top down\" approach starting with the entire dataset and then finding partitions in a stepwise manner. Agglomerative clustering is a \"bottom up\" approach. In this lab you will work with agglomerative clustering which works as follows:\n",
    "\n",
    "1. The linkage distances between each of the data points are computed.\n",
    "2. Points are clustered pairwise with their nearest neighbor.\n",
    "3. Linkage distances between the clusters are computed.\n",
    "4. Clusters are combined pairwise into larger clusters.\n",
    "5. Steps 3 and 4 are repeated until all data points are in a single cluster.\n",
    "\n",
    "The linkage function can be computed in a number of ways:\n",
    "- *Ward* linkage measures the increase in variance for the clusters being linked.\n",
    "- *Average* linkage uses the mean pairwise distance between the members of the two clusters.\n",
    "- *Complete* or *maximal* linkage uses the maximum distance between the members of the two clusters.\n",
    "\n",
    "Several different distance metrics are used to compute linkage functions:\n",
    "- *Euclidean* or *l2* distance is the most widely used. This metric is only choice for the Ward linkage method.\n",
    "- *Manhattan* or *l1* distance is robust to outliers and has other interesting properties.\n",
    "- *Cosine similarity* is the dot product between the location vectors, divided by the magnitudes of the vectors. Note that this metric is a measure of similarity, whereas the other two metrics are measures of difference. Similarity can be quite useful when working with data such as images or text documents.\n"
   ],
   "id": "3241fdb14e28d3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "##### Agglomerative Clustering\n",
    "\n",
    "Let's see an example of clustering the seeds data using an agglomerative clustering algorithm."
   ],
   "id": "ae83bf36ed895d67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "agg_model = AgglomerativeClustering(n_clusters=3)\n",
    "agg_clusters = agg_model.fit_predict(features.values)\n",
    "print(agg_clusters)"
   ],
   "id": "b20eb90da9165b56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_clusters(features_2d, agg_clusters, \"Agglomerative Clustering Assignments\")\n",
   "id": "8802b1271d575fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To calculate the silhouette score for the agglomerative clustering model:",
   "id": "f666f0c331dc0044"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "score = silhouette_score(features.values, agg_clusters)\n",
    "print(f\"Silhouette Score: {score:.3f}\")\n",
    "\n",
    "# Interpretation\n",
    "if score > 0.5:\n",
    "    print(\"✓ Strong cluster separation - clusters are well-defined\")\n",
    "elif score > 0.3:\n",
    "    print(\"→ Reasonable cluster structure - acceptable separation\")\n",
    "else:\n",
    "    print(\"✗ Weak or overlapping clusters - consider adjusting k or reviewing data\")"
   ],
   "id": "6c60a8ea56c58926",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Summary\n",
    "\n",
    "Here we practiced using K-Means and hierarchical clustering. This unsupervised learning has the ability to take unlabelled data and identify which data points are similar to others."
   ],
   "id": "a52d58f57dd19b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
